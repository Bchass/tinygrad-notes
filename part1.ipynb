{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: handle multiplication on CPU\n",
    "\n",
    "Let's implement an autograd library that can handle multiplication:\n",
    "\n",
    "a = Tensor(np.array([2]))\n",
    "b = Tensor(np.array([3]))\n",
    "c = a.mul(b)\n",
    "c.backward()\n",
    "\n",
    "We will expect a.grad to be [3] and b.grad = to be [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use numpy as the underlying library to handle the actual operations, what we build here is how to calculate gradient automatically. Let's first import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with a Tensor class that takes numpy array as input, it will have the mul and backward method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"<Tensor with data: {self.data}, grad: {self.grad}>\"\n",
    "  \n",
    "  def mul(self, other):\n",
    "    pass\n",
    "  \n",
    "  def backward(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplication method will be handled by just calling the numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(self, other):\n",
    "    out_data = self.data * other.data # np.array([2]) * np.array([3])\n",
    "    out_tensor = Tensor(out_data)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having just the calculation is not enough, we need to keep track of the operations so we can do backpropagation. The idea is to pass in the the two tensors that formed the new tensor as an argument, and then do a recursive search for the calculation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, data, children=()) -> None:\n",
    "    self.data = data\n",
    "    self._prev = set(children)\n",
    "\n",
    "  def mul(self, other):\n",
    "    out = Tensor(self.data * other.data, (self, other))\n",
    "    return out\n",
    "  \n",
    "  def build_topo(self):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def _build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          _build_topo(child)\n",
    "        topo.append(v)\n",
    "    _build_topo(self)\n",
    "    return topo\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"<Tensor with data: {self.data}>\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new tensor, we can actually show the calculation graph. Note that I added a __repr__ method to show the content of this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Tensor with data: [3]>, <Tensor with data: [2]>, <Tensor with data: [6]>]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.array([2]))\n",
    "b = Tensor(np.array([3]))\n",
    "c = a.mul(b)\n",
    "tree = c.build_topo()\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try with more complicated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Tensor with data: [3]>, <Tensor with data: [2]>, <Tensor with data: [6]>, <Tensor with data: [18]>, <Tensor with data: [108]>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = Tensor(np.array([2]))\n",
    "b = Tensor(np.array([3]))\n",
    "c = a.mul(b)\n",
    "d = c.mul(b)\n",
    "e = d.mul(c)\n",
    "tree = e.build_topo()\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is a mechanism to record all the tensors that contributed to the final value, however, the relationship between how those tensors are operated are not kept. Next, we will implement the actual backpropagation to fill in that missing piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we add two more attributes to the tensor class. One is grad, this records the current gradient of this tensor, each tensor will have a _backward attributes as a function that perform updates to its own gradient. This function is only defined when operations like multiplication is performed on it, hence capturing the relationship between the two tensors that were combined to form it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, data, children=()) -> None:\n",
    "    self.data = data\n",
    "    self._prev = set(children)\n",
    "    self.grad = np.zeros(self.data.shape)\n",
    "    self._backward = lambda: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's see how the `mul` method can capture the 3 * 2 operations and update the gradient accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, data, children=()) -> None:\n",
    "    self.data = data\n",
    "    self._prev = set(children)\n",
    "    self.grad = np.zeros(self.data.shape)\n",
    "    self._backward = lambda: None\n",
    "\n",
    "  def mul(self, other):\n",
    "    out = Tensor(self.data * other.data, (self, other))\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"<Tensor with data: {self.data}, grad: {self.grad}>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with this to understand what it does. In the following example, c = a * b, meaning that the gradient of a is the .data on b, and the gradient of b is the .data on a. If we want to have the value show up on the a and b instance, we can call c._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor with data: [2], grad: [0.]>\n",
      "<Tensor with data: [3], grad: [0.]>\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.array([2]))\n",
    "b = Tensor(np.array([3]))\n",
    "c = a.mul(b)\n",
    "c._backward()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, it's not what we expect, reason is that the initial gradient of c is zero, so when you multiply it by other values, it is still zero. However, the initial gradient of a and b needs to be 0. To fix it, we initialize the gradient of our \"starting\" tensor with 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor with data: [2], grad: [3.]>\n",
      "<Tensor with data: [3], grad: [2.]>\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.array([2]))\n",
    "b = Tensor(np.array([3]))\n",
    "c = a.mul(b)\n",
    "c.grad = np.ones(c.data.shape)\n",
    "c._backward()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the expected result. In fact, we have already implemented the backpropgation, we just didn't wrap it into a proper method and have it applied on every tensor, let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor with data: [2 3], grad: [ 9. 16.]>\n",
      "<Tensor with data: [3 4], grad: [12. 24.]>\n"
     ]
    }
   ],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, data, children=()) -> None:\n",
    "    self.data = data\n",
    "    self._prev = set(children)\n",
    "    self.grad = np.zeros(self.data.shape)\n",
    "    self._backward = lambda: None\n",
    "\n",
    "  def mul(self, other):\n",
    "    out = Tensor(self.data * other.data, (self, other))\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "    \n",
    "  def build_topo(self):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def _build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          _build_topo(child)\n",
    "        topo.append(v)\n",
    "    _build_topo(self)\n",
    "    return topo\n",
    "\n",
    "  def backward(self):\n",
    "    self.grad = np.ones(self.data.shape)\n",
    "    tree = self.build_topo()\n",
    "    for tensor in reversed(tree):\n",
    "      tensor._backward()\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"<Tensor with data: {self.data}, grad: {self.grad}>\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result now, as you can see, it handles multi element tensors and multiple operations well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([2,3]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a.mul(b)\n",
    "d = c.mul(b)\n",
    "d.backward()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an autograd library in its simpliest form. Recall in the title I said we are making this run on CPU? that's actually numpy. When we do `self.data * other.grad`, the __mul__ method is provided by the numpy library, which runs on CPU in the form of JIT compiled C++ code. Because the __mul__ happens on numpy, we don't have to worry it about it polluting our _backward function that's updated with every call to `.mul()`. In order for this to work on GPU, and allows for easy transitioning between the two device type, we will need to have something like numpy, but running on GPU. There are different ways to do this, and in the next notebook, I will present the approach tinygrad has taken."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
