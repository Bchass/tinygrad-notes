# Beam search explained

One key feature of tinygrad, or any ML framework, is to generate efficient kernel code. For example, a naive implementation
of a sum on dimension 0:

```python
Tensor.empty(4, 4).sum(0).realize()
```

may result in such kernel code:

```c++
kernel void r_4_4(device float* data0, device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {
  int gidx0 = gid.x; /* 4 */
  float acc0 = 0.0f;
  for (int ridx0 = 0; ridx0 < 4; ridx0++) {
    float val0 = *(data1+(gidx0+(ridx0<<2)));
    acc0 = (acc0+val0);
  }
  *(data0+gidx0) = acc0;
}
```

This can be optimized by taking away the for loop, and handle it inline:

```c++
kernel void r_4_4n2(device float* data0, device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {
  int gidx0 = gid.x; /* 4 */
  float val0 = *(data1+gidx0);
  float val1 = *(data1+(gidx0+4));
  float val2 = *(data1+(gidx0+8));
  float val3 = *(data1+(gidx0+12));
  *(data0+gidx0) = (val3+val2+val0+val1);
}
```

Alternatively, we can launch just one kernel and let it process all four columns:

```c++
kernel void r_4_4n2(device float* data0, device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {
  float acc0 = 0.0f;
  float acc1 = 0.0f;
  float acc2 = 0.0f;
  float acc3 = 0.0f;
  for (int ridx0 = 0; ridx0 < 4; ridx0++) {
    float4 val0 = *((device float4*)((data1+(ridx0<<2))));
    acc0 = (acc0+val0.x);
    acc1 = (acc1+val0.y);
    acc2 = (acc2+val0.z);
    acc3 = (acc3+val0.w);
  }
  *((device float4*)((data0+0))) = float4(acc0,acc1,acc2,acc3);
}
```

How do we know which one is faster? There are two approaches in general, one is hand code some
heuristics based on experience. There are lots of articles online that explain how to make your matrix multiplication
kernel go faster, such as [this one](https://siboehm.com/articles/22/CUDA-MMM). Another approach is by automatically searching over
all the possible optimization techniques and their parameters, compare the speed, and pick the fastest one. The search algorithm
currently used is beam search, and in fact is just the specific example of the general search approaches (among others there are
monte carlo tree search, or even neural net search, etc.). As pointed out by 
[Sutton's "The Bitter Lesson"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html),
the search approach will likely to triumph.

That's actually all there is for the beam search! When a kernel is generated, it goes through all the possible optimization
and measure the speed, then pick the quickest one. Quite simple.

The more complicated part is the options. In order for search to work, we need to define a set of optimization for it to search over. What
I have illustrated in the above examples, are "UNROLL" and "UPCAST", and there are many more.

## UNROLL

