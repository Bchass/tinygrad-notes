{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Making it run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last post presented an autograd library in its simpliest form. Recall in the title of that post I said we were making this run on CPU? that's actually numpy. When we do `self.data * other.grad`, the __mul__ method is provided by the numpy library, which runs on CPU in the form of JIT compiled C++ code. Because the __mul__ happens on numpy, we don't have to worry it about it polluting our _backward function that's updated with every call to `.mul()`. In order for this to work on GPU, and allows for easy transitioning between the two device type, we will need to have something like numpy, but running on GPU. There are different ways to do this, and in the this notebook, I will present the approach tinygrad has taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running things on GPU requires us to write some GPU specific code, I'll use pyopenCL. First let's look at how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.]\n"
     ]
    }
   ],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "\n",
    "# Boilerplate for openCL\n",
    "cl_ctx = cl.create_some_context(answers=[0,2])  # change if you don't have mac\n",
    "cl_queue = cl.CommandQueue(cl_ctx)\n",
    "\n",
    "# First create the value in numpy  that lives on CPU\n",
    "a = np.array([2], dtype=np.float32)\n",
    "b = np.array([3], dtype=np.float32)\n",
    "\n",
    "# Then convert them into openCL buffer that lives on the GPU\n",
    "a = cl.Buffer(cl_ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=a)\n",
    "b = cl.Buffer(cl_ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=b)\n",
    "\n",
    "# Create the destination buffer (output value). This may seem weird if you come from an interpreted language background\n",
    "# but it is common in low level programming to first allocate a variable/space before writing data to it\n",
    "c = cl.Buffer(cl_ctx, cl.mem_flags.WRITE_ONLY, 4) # output is a single integer, 1 integer takes up 4 bytes\n",
    "\n",
    "# Write the actual openCL GPU code that would do the calculation\n",
    "prg = cl.Program(cl_ctx, \"\"\"\n",
    "    __kernel void mul(\n",
    "        __global const float *a_g, __global const float *b_g, __global float *res_g)\n",
    "    {\n",
    "      int gid = get_global_id(0);\n",
    "      res_g[gid] = a_g[gid] * b_g[gid];\n",
    "    }\n",
    "    \"\"\").build()\n",
    "\n",
    "# The .build() step will create a method of the same name on the prg instance, and we execute it\n",
    "prg.mul(cl_queue, [c.size//4], None, a, b, c)\n",
    "\n",
    "# Data on GPU is not directly accessible, we have to copy it to numpy in order to view it\n",
    "# So again, allocate a variable and then paste the content into it with the enqueue_copy method\n",
    "data = np.empty((1,), dtype=np.float32)\n",
    "cl.enqueue_copy(cl_queue, data, c)\n",
    "print(data) # output [6.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to replace the `*` and `+` (__mul__ and __add__) methods from part 1 with the operations above. We can keep most things as is, and add the __mul__ and __add__ methods. Let's implement __mul__ first. \n",
    "\n",
    "Two things to note:\n",
    "1. with the added implementation, .data attribute is an openCL buffer rather than numpy, so places that contain numpy code need to be modified so the types are consistent.\n",
    "\n",
    "2. __mul__ and __add__ happens directly on the tensor, rather than on .data, therefore, .grad also need to be a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor with data: <pyopencl._cl.Buffer object at 0x110a72990>, grad: <Tensor with data: <pyopencl._cl.Buffer object at 0x110a73650>, grad: <Tensor with data: <pyopencl._cl.Buffer object at 0x110a73770>, grad: None>>>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "# Boilerplate for openCL\n",
    "cl_ctx = cl.create_some_context(answers=[0,2])  # change if you don't have mac\n",
    "cl_queue = cl.CommandQueue(cl_ctx)\n",
    "class Tensor:\n",
    "  def __mul__(self, other):\n",
    "    out = cl.Buffer(cl_ctx, cl.mem_flags.WRITE_ONLY, 4) # output is a single integer, 1 integer takes up 4 bytes\n",
    "\n",
    "    # Write the actual openCL GPU code that would do the calculation\n",
    "    prg = cl.Program(cl_ctx, \"\"\"\n",
    "        __kernel void mul(\n",
    "            __global const float *a_g, __global const float *b_g, __global float *res_g)\n",
    "        {\n",
    "          int gid = get_global_id(0);\n",
    "          res_g[gid] = a_g[gid] * b_g[gid];\n",
    "        }\n",
    "        \"\"\").build()\n",
    "\n",
    "    # The .build() step will create a method of the same name on the prg instance, and we execute it\n",
    "    prg.mul(cl_queue, [out.size//4], None, self.data, other.data, out)\n",
    "    return Tensor(out, (self, other))\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    out = cl.Buffer(cl_ctx, cl.mem_flags.WRITE_ONLY, 4) # output is a single integer, 1 integer takes up 4 bytes\n",
    "\n",
    "    # Write the actual openCL GPU code that would do the calculation\n",
    "    prg = cl.Program(cl_ctx, \"\"\"\n",
    "        __kernel void add(\n",
    "            __global const float *a_g, __global const float *b_g, __global float *res_g)\n",
    "        {\n",
    "          int gid = get_global_id(0);\n",
    "          res_g[gid] = a_g[gid] + b_g[gid];\n",
    "        }\n",
    "        \"\"\").build()\n",
    "\n",
    "    # The .build() step will create a method of the same name on the prg instance, and we execute it\n",
    "    prg.add(cl_queue, [out.size//4], None, self.data, other.data, out)\n",
    "    return Tensor(out, (self, other))\n",
    "  \n",
    "  def __init__(self, data, children=(), requires_grad=True) -> None:\n",
    "    self.data = data\n",
    "    self._prev = set(children)\n",
    "\n",
    "    # Convert grad to openCL buffer\n",
    "    grad = np.zeros(1)\n",
    "    grad = cl.Buffer(cl_ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=grad)\n",
    "\n",
    "    self.grad = Tensor(grad, requires_grad=False) if requires_grad else None\n",
    "\n",
    "    self._backward = lambda: None\n",
    "\n",
    "  def mul(self, other):\n",
    "    out = self * other\n",
    "    def _backward():\n",
    "      self.grad += other * out.grad\n",
    "      other.grad += self * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "    \n",
    "  def build_topo(self):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def _build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          _build_topo(child)\n",
    "        topo.append(v)\n",
    "    _build_topo(self)\n",
    "    return topo\n",
    "\n",
    "  def backward(self):\n",
    "\n",
    "    # Convert to openCL buffer\n",
    "    grad = np.ones((1,))\n",
    "    grad = cl.Buffer(cl_ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=grad)\n",
    "    self.grad = Tensor(grad)\n",
    "    tree = self.build_topo()\n",
    "    for tensor in reversed(tree):\n",
    "      tensor._backward()\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"<Tensor with data: {self.data}, grad: {self.grad}>\"\n",
    "  \n",
    "\n",
    "a = cl.Buffer(cl_ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=np.array([2]))\n",
    "a = Tensor(a)\n",
    "b = cl.Buffer(cl_ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=np.array([3]))\n",
    "b = Tensor(b)\n",
    "c = a.mul(b)\n",
    "d = c.mul(b)\n",
    "d.backward()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, we managed to get the operation to run on GPU without crashing it, but there are a few important issues:\n",
    "1. If you examine the ouput, the grad is a tensor with grad and nested a few layers deep, this should be fixed to make it consistent because the gradient itself should not have gradient. In other words, we need to have a mechanism that determine if gradient should be tracked or not across the board.\n",
    "\n",
    "2. In our cpu implementation, we used Tensor.data.shape to decide the datashape, however, GPU buffer doesn't have this attribute, so I took a shortcut and just assumed the shape is just 1.\n",
    "\n",
    "3. We have to initialize the data first as the buffer, and then construct the tensor, this is fine at first glance, but if you examine the operation in __mul__ and __add__, they need to be handled meticulously as well.\n",
    "\n",
    "First change we will make is to specify whether the Tensor need gradient or not, and have it store this attributes. The gradient will always be set to None at the beginning\n",
    "\n",
    "```python\n",
    "def __init__(self, data, device=Device.DEFAULT, requires_grad=True):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
