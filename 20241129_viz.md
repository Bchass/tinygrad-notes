# Understanding the VIZ=1 tool

Many frameworks talks about optimization as some form of archane spell that you should not worry about, but
tinygrad makes that explicit and comes with a visualization tool, allowing you to pry under the hood easily:

<img src="images/51.png">

## Loop unrolling example

I will use a loop unrolling example to illustrate the tool:

```python
from tinygrad import Tensor
a = Tensor.empty(4)
a.sum(0).realize()
```

First we see what the result looks like without loop unrolling: `DEBUG=5 NOOPT=1 python script.py`, the DEBUG flag
will allow us to see the generated code, and NOOPT tells tinygrad not to enable optimization. In the output you should
see the generated metal/cuda code:

```c++
kernel void r_4(device float* data0, device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {
  float acc0 = 0.0f;
  for (int ridx0 = 0; ridx0 < 4; ridx0++) {
    float val0 = *(data1+ridx0);
    acc0 = (acc0+val0);
  }
  *(data0+0) = acc0;
}
```

The sum is managed by a loop that iterates four times. We know that for small numbers like this, we can take advantage 
of vectorized data type, and remove the loop. In fact, that's what the optimization does, let's run it with optimization
on this time `DEBUG=5 python script.py`:

```c++
kernel void r_4(device float* data0, device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {
  float4 val0 = *((device float4*)((data1+0)));
  *(data0+0) = (val0.w+val0.z+val0.x+val0.y);
}
```

## VIZ=1 

To use the tool: `VIZ=1 NOOPT=1 python script.py`, the VIZ=1 flag will cause tinygrad to save all the pattern matching
history, and start a web server upon script exit. You can see my other post on [pattern matching](20241112_pm.md). I'm 
using the NOOPT=1 flag to help us dissect the "before" of the optimization. You should see something similar to the first
picture of this post. 

<img src="images/img52.png">

On the left column, it shows "scheduler" patterns and a list of kernels. I'll skip the scheduler part for now.
For kernels in our case, we only have one kernel called `r_4`, clicking on it gives you this:

<img src="images/img53.png">

Let's look at the center part, it's an AST representating our "intended computation": sum four numbers that are stored
on some buffer, and store the result in another buffer. On the far right end, we have a "SINK", this is an arbitrary way
of saying the computation ends here. It's parent is a "STORE" node, we can guess this means store the computed result. To
store something, we need three pieces of information, where to store, how to store, and what to store. Intuitively, this 
maps to the three parents it has. We can deduce that "DEFINE_GLOBAL" with argument 0 would be the "where", and it maps to
`device float* data0` in our generated code. The "VIEW" node is the how. We see that the generated code, we are storing it
as `*(data0+0)`, meaning that there's only one element. The shape looking code corrobarates this, as its shape is `(1,)`.
The "what" is what comes after the "=" sign in the C++ code, and its the result of a previous computation. In the graph,
it is a "REDUCE_AXIS" node. It has an argument 0, this refers to the fact we are doing a reduce op in the 0th axis, which
makes sense since our input tensor has just 1 dimension. Furthermore, it has an op called "ADD", this also makes sense,
because we are summing things up. Notice how this doesn't translate to the loop and accumulator. This is because at this
stage we have just a high level representation, it is up to the optimizer to decide how this should be accomplished. As
such, this graph will look almost identical should you have optimization on. Regardless of whether you do float4 vector 
or a loop, the high level operation is alawys "REDUCE_AXIS on axis 0, with op ADD". 

